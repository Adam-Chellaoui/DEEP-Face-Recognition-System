{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20efbd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Images directories\n",
    "train_dir = './train_images'\n",
    "test_dir = './test_images'\n",
    "\n",
    "# Normalize data -> prepare transformer\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Grayscale(), \n",
    "     transforms.ToTensor(), \n",
    "     transforms.Normalize(mean=(0,),std=(1,))])\n",
    "\n",
    "# Import data and transform them\n",
    "train_data = torchvision.datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_data = torchvision.datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "# Size of training / data -> 20% goes to validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# When train ML, send batch by batch -> we send packages (batch) of 32 images every step\n",
    "batch_size = 32\n",
    "\n",
    "# Get index according to length, and shuffle it -> separate validation and training randomly every time\n",
    "num_train = len(train_data)\n",
    "indices_train = list(range(num_train))\n",
    "np.random.shuffle(indices_train)\n",
    "\n",
    "# Get index used for validation by spliting array\n",
    "split_tv = int(np.floor(valid_size * num_train))\n",
    "train_new_idx, valid_idx = indices_train[split_tv:],indices_train[:split_tv]\n",
    "\n",
    "# Sampler function\n",
    "# train_sampler = SubsetRandomSampler(train_new_idx)\n",
    "# valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_sampler = ImbalancedDatasetSampler(train_data, train_new_idx)\n",
    "valid_sampler = ImbalancedDatasetSampler(train_data, valid_idx)\n",
    "\n",
    "# What to feed to training\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=1)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "classes = ('noface','face')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f0a93c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Network, ready to use\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, padding=2, padding_mode='zeros')\n",
    "        self.pool = nn.MaxPool2d(2, 2) #Pooling 2 by 2, window of 2\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5 , padding=2, padding_mode='zeros')\n",
    "        #self.conv3 = nn.Conv2d(64, 128, 5, padding=2, padding_mode='zeros')\n",
    "        self.fc1 = nn.Linear(64 * 9 * 9, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 32)\n",
    "        self.fc5 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        #x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, 64 * 9 * 9)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dbeb7480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network creation\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cf0f2dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Entropy error -> test accuracy of model, we want criterion to tend to 0.\n",
    "# Optimize change network each time to make it better. It's ran between every batches\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb941dab",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "08e9d85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.693\n",
      "[1,   400] loss: 0.693\n",
      "[1,   600] loss: 0.692\n",
      "[1,   800] loss: 0.692\n",
      "[1,  1000] loss: 0.691\n",
      "[1,  1200] loss: 0.690\n",
      "[1,  1400] loss: 0.687\n",
      "[1,  1600] loss: 0.682\n",
      "[1,  1800] loss: 0.668\n",
      "[1,  2000] loss: 0.585\n",
      "[1,  2200] loss: 0.418\n",
      "[2,   200] loss: 0.267\n",
      "[2,   400] loss: 0.222\n",
      "[2,   600] loss: 0.177\n",
      "[2,   800] loss: 0.178\n",
      "[2,  1000] loss: 0.127\n",
      "[2,  1200] loss: 0.126\n",
      "[2,  1400] loss: 0.097\n",
      "[2,  1600] loss: 0.090\n",
      "[2,  1800] loss: 0.074\n",
      "[2,  2000] loss: 0.068\n",
      "[2,  2200] loss: 0.055\n",
      "[3,   200] loss: 0.058\n",
      "[3,   400] loss: 0.061\n",
      "[3,   600] loss: 0.047\n",
      "[3,   800] loss: 0.044\n",
      "[3,  1000] loss: 0.035\n",
      "[3,  1200] loss: 0.044\n",
      "[3,  1400] loss: 0.041\n",
      "[3,  1600] loss: 0.034\n",
      "[3,  1800] loss: 0.032\n",
      "[3,  2000] loss: 0.027\n",
      "[3,  2200] loss: 0.022\n",
      "[4,   200] loss: 0.024\n",
      "[4,   400] loss: 0.027\n",
      "[4,   600] loss: 0.021\n",
      "[4,   800] loss: 0.029\n",
      "[4,  1000] loss: 0.023\n",
      "[4,  1200] loss: 0.025\n",
      "[4,  1400] loss: 0.030\n",
      "[4,  1600] loss: 0.023\n",
      "[4,  1800] loss: 0.019\n",
      "[4,  2000] loss: 0.024\n",
      "[4,  2200] loss: 0.013\n",
      "[5,   200] loss: 0.022\n",
      "[5,   400] loss: 0.020\n",
      "[5,   600] loss: 0.019\n",
      "[5,   800] loss: 0.023\n",
      "[5,  1000] loss: 0.014\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-7ea3a257f96e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saved_loss = []\n",
    "for epoch in range(10):  # loop over the dataset multiple times -> epoch = nb of time we go through dataset, careful with overfitting\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward (calculating via current model) + backward (learning from mistakes) + optimize (fix mistakes)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item() # We want to minimize loss\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
    "            if i == 2199:\n",
    "                saved_loss.append(running_loss)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec7ca5d",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3491937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 95 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e543521",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, classes = next(iter(test_loader))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "328205ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a670d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a43cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "image_tensor = torchvision.utils.make_grid(images[0])\n",
    "image_label = labels[0]\n",
    "\n",
    "# show images\n",
    "imshow(image_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc917a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f616d7d3",
   "metadata": {},
   "source": [
    "### Printing loss according to epoch, to find best model and avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11a6bc6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# X axis parameter:\n",
    "xaxis = range(0, 10)\n",
    "\n",
    "# Y axis parameter:\n",
    "yaxis = saved_loss \n",
    "\n",
    "plt.plot(xaxis, yaxis, 'ro')\n",
    "plt.grid(color = 'green', linestyle = '--', linewidth = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d777f98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "0673ab36cf4981d9f173f29ef63de73d925b2d0199ec09b3cce3aea6166e893b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
